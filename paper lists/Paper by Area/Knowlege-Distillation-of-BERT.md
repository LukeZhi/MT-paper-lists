# Knowledge Distillation of BERT

| Paper                                                        | Authors                                                      | Venue        | Link                                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------ | --------------------------------------------- |
| Patient Knowledge Distillation for BERT Model Compression    | *Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu*                  | EMNLP 2019   | https://www.aclweb.org/anthology/D19-1441.pdf |
| ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | *Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut* | ICLR 2020    | https://arxiv.org/abs/1909.11942              |
| TinyBERT: Distilling BERT for Natural Language Understanding | *Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu* | arxiv        | https://arxiv.org/abs/1909.10351              |
| DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | *Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf*  | NeurlPS 2019 | https://arxiv.org/abs/1910.01108              |

